{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368b90e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "bayes_opt_cifar10.py\n",
    "\n",
    "A reproducible, modular example that:\n",
    "- Defines a small, parameterized CNN for CIFAR-10 (PyTorch).\n",
    "- Uses scikit-optimize (skopt) for Bayesian Optimization (gp_minimize).\n",
    "- Compares to a Random Search baseline.\n",
    "- Logs validation accuracy and saves best model.\n",
    "- Designed to \"run without error\" if dependencies are installed.\n",
    "\n",
    "Usage:\n",
    "    pip install -r requirements.txt\n",
    "    python bayes_opt_cifar10.py\n",
    "\n",
    "Requirements (example):\n",
    "    torch>=1.8, torchvision, scikit-optimize, numpy, tqdm\n",
    "\n",
    "Author: ChatGPT (GPT-5 Thinking mini)\n",
    "Date: 2025-11-19\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "from skopt.utils import use_named_args\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------\n",
    "# Configuration / Globals\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"./data\"\n",
    "BEST_MODEL_PATH = \"best_cifar10_model.pth\"\n",
    "NUM_CLASSES = 10\n",
    "DEFAULT_EPOCHS = 12  # keep small by default to run quickly for testing\n",
    "WORKERS = 4 if torch.cuda.is_available() else 0\n",
    "\n",
    "# -----------------------\n",
    "# Determinism\n",
    "# -----------------------\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Make CUDA deterministic â€” might affect performance\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Data helpers\n",
    "# -----------------------\n",
    "def get_dataloaders(\n",
    "    batch_size: int = 128, val_fraction: float = 0.1\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Returns train, val, test dataloaders for CIFAR-10.\n",
    "\n",
    "    Args:\n",
    "        batch_size: batch size for loaders.\n",
    "        val_fraction: fraction of training set used for validation.\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    transform_train = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ]\n",
    "    )\n",
    "    transform_test = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    full_train = torchvision.datasets.CIFAR10(\n",
    "        root=DATA_DIR, train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root=DATA_DIR, train=False, download=True, transform=transform_test\n",
    "    )\n",
    "\n",
    "    val_size = int(len(full_train) * val_fraction)\n",
    "    train_size = len(full_train) - val_size\n",
    "    train_set, val_set = random_split(\n",
    "        full_train, [train_size, val_size], generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=WORKERS)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=WORKERS)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A small configurable convolutional network for CIFAR-10.\"\"\"\n",
    "\n",
    "    def __init__(self, num_filters: int = 32, num_conv_layers: int = 3, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        Create a simple CNN.\n",
    "\n",
    "        Args:\n",
    "            num_filters: base number of filters in first conv layer.\n",
    "            num_conv_layers: how many conv blocks (each block may double filters).\n",
    "            dropout: dropout probability before final classifier.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_ch = 3\n",
    "        out_ch = num_filters\n",
    "        for i in range(num_conv_layers):\n",
    "            layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "            in_ch = out_ch\n",
    "            out_ch = min(out_ch * 2, 512)\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        # compute flattened feature size for CIFAR-10 (32x32) after pooling each block halves dims\n",
    "        final_spatial = 32 // (2 ** num_conv_layers)\n",
    "        final_channels = in_ch\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(final_channels * final_spatial * final_spatial, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Training + Evaluation\n",
    "# -----------------------\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    \"\"\"Train model for one epoch; return average training loss.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = x.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        total += batch_size\n",
    "    return running_loss / total if total else 0.0\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model: returns (avg_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    avg_loss = running_loss / total if total else 0.0\n",
    "    acc = correct / total if total else 0.0\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Objective for Bayesian Optimization\n",
    "# -----------------------\n",
    "# Define the hyperparameter search space\n",
    "search_space = [\n",
    "    Real(1e-4, 1e-1, prior=\"log-uniform\", name=\"lr\"),\n",
    "    Integer(2, 4, name=\"num_conv_layers\"),  # keeps model small\n",
    "    Integer(16, 128, name=\"num_filters\"),\n",
    "    Real(0.0, 0.5, name=\"dropout\"),\n",
    "]\n",
    "\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def objective(**params) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Bayesian optimization.\n",
    "    Returns negative validation accuracy (since skopt minimizes).\n",
    "    \"\"\"\n",
    "    # Unpack parameters\n",
    "    lr = float(params[\"lr\"])\n",
    "    num_conv_layers = int(params[\"num_conv_layers\"])\n",
    "    num_filters = int(params[\"num_filters\"])\n",
    "    dropout = float(params[\"dropout\"])\n",
    "\n",
    "    # Fixed training parameters for speed / reproducibility\n",
    "    epochs = DEFAULT_EPOCHS\n",
    "    batch_size = 128\n",
    "\n",
    "    # Prepare data and model\n",
    "    train_loader, val_loader, _ = get_dataloaders(batch_size=batch_size)\n",
    "    model = SimpleCNN(num_filters=num_filters, num_conv_layers=num_conv_layers, dropout=dropout).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    # small training loop\n",
    "    for epoch in range(epochs):\n",
    "        _train_loss = train_one_epoch(model, optimizer, criterion, train_loader, DEVICE)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, DEVICE)\n",
    "        scheduler.step()\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        # Optional: early stopping (if convergence)\n",
    "        # If training is noisy for small epochs, we avoid aggressive stopping.\n",
    "    # We minimize so return negative validation accuracy\n",
    "    return -best_val_acc\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Random Search Baseline\n",
    "# -----------------------\n",
    "def random_search_trials(n_trials: int = 10):\n",
    "    \"\"\"\n",
    "    Run random sampling across the same search space. Returns best validation accuracy and params.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    best_acc = 0.0\n",
    "    best_params = None\n",
    "    for i in range(n_trials):\n",
    "        # sample uniformly in log space for lr\n",
    "        lr = float(10 ** rng.uniform(np.log10(1e-4), np.log10(1e-1)))\n",
    "        num_conv_layers = int(rng.randint(2, 5))  # upper bound is exclusive\n",
    "        num_filters = int(rng.randint(16, 129))\n",
    "        dropout = float(rng.uniform(0.0, 0.5))\n",
    "        # evaluate\n",
    "        params = {\"lr\": lr, \"num_conv_layers\": num_conv_layers, \"num_filters\": num_filters, \"dropout\": dropout}\n",
    "        acc = -objective(**params)  # objective returns -val_acc\n",
    "        print(f\"[Random] Trial {i+1}/{n_trials} - val_acc={acc:.4f} params={params}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = params\n",
    "    return best_acc, best_params\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Run optimization and compare\n",
    "# -----------------------\n",
    "def run_optimization(n_calls: int = 12, random_search_trials_count: int = 6) -> None:\n",
    "    \"\"\"\n",
    "    Run Bayesian Optimization and Random Search baseline, save best model found by Bayesian opt.\n",
    "    \"\"\"\n",
    "    print(\"Starting Bayesian Optimization (skopt.gp_minimize)...\")\n",
    "    start_time = time.time()\n",
    "    # gp_minimize will run the objective function n_calls times (including initial points).\n",
    "    res = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=search_space,\n",
    "        acq_func=\"EI\",  # expected improvement\n",
    "        n_calls=n_calls,\n",
    "        n_initial_points=4,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    bo_time = time.time() - start_time\n",
    "    best_index = res.x_iters.index(res.x)\n",
    "    best_params = dict(lr=res.x[0], num_conv_layers=int(res.x[1]), num_filters=int(res.x[2]), dropout=float(res.x[3]))\n",
    "    best_val_acc = -res.fun  # because we minimized negative val acc\n",
    "    print(f\"Bayes Opt finished in {bo_time:.1f}s. Best val acc: {best_val_acc:.4f}\")\n",
    "    print(f\"Best params (BO): {best_params}\")\n",
    "\n",
    "    # Run a short random search baseline\n",
    "    print(\"\\nStarting Random Search baseline...\")\n",
    "    rs_start = time.time()\n",
    "    rs_best_acc, rs_best_params = random_search_trials(n_trials=random_search_trials_count)\n",
    "    rs_time = time.time() - rs_start\n",
    "    print(f\"Random Search finished in {rs_time:.1f}s. Best val acc: {rs_best_acc:.4f}\")\n",
    "    print(f\"Best params (Random): {rs_best_params}\")\n",
    "\n",
    "    # As a final step, train a model with best BO params for a bit longer and save it\n",
    "    print(\"\\nTraining final model with BO best params and saving to disk...\")\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(batch_size=128)\n",
    "    final_model = SimpleCNN(\n",
    "        num_filters=int(best_params[\"num_filters\"]),\n",
    "        num_conv_layers=int(best_params[\"num_conv_layers\"]),\n",
    "        dropout=float(best_params[\"dropout\"]),\n",
    "    ).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(final_model.parameters(), lr=float(best_params[\"lr\"]), momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    final_epochs = max(DEFAULT_EPOCHS, 20)  # train a bit longer for final model\n",
    "    best_val = 0.0\n",
    "    for epoch in range(final_epochs):\n",
    "        train_loss = train_one_epoch(final_model, optimizer, criterion, train_loader, DEVICE)\n",
    "        val_loss, val_acc = evaluate(final_model, val_loader, DEVICE)\n",
    "        scheduler.step()\n",
    "        print(f\"[Final] Epoch {epoch+1}/{final_epochs} train_loss={train_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "    print(f\"Final model saved to {BEST_MODEL_PATH} with val acc {best_val:.4f}\")\n",
    "\n",
    "    # Evaluate saved model on test set\n",
    "    print(\"\\nEvaluating saved model on test set...\")\n",
    "    final_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))\n",
    "    test_loss, test_acc = evaluate(final_model, test_loader, DEVICE)\n",
    "    print(f\"Test loss: {test_loss:.4f} Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Summarize\n",
    "    print(\"\\nSUMMARY:\")\n",
    "    print(f\"BO best val acc: {best_val_acc:.4f} params: {best_params}\")\n",
    "    print(f\"Random best val acc: {rs_best_acc:.4f} params: {rs_best_params}\")\n",
    "    print(f\"BO runtime: {bo_time:.1f}s  Random runtime: {rs_time:.1f}s\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Entrypoint\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Keep runtime reasonable by default. User can edit n_calls / trials above.\n",
    "    run_optimization(n_calls=12, random_search_trials_count=6)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
